## 线性回归
1. 基本形式$$ f(\boldsymbol{x})=w_1x_1+w_2x_2+\ldots+w_dx_d+b $$
   用向量形式表示为：$$ 
    f(\boldsymbol{x})=\boldsymbol{w}^\mathrm{T}\boldsymbol{x}+b
    $$
2. 线性模型具有很好的可解释性，每个变量的权重都可以得到解释。   
3. 广义上觉得线性模型(generalized linear model)
$$y=g^{-1}(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}+b) $$
其中g(·)成为联系函数。
## 对数几率回归
![单位阶跃函数](https://z4a.net/images/2023/12/04/420beb5413b2aa35e9ba4901f394cd92.png)
用simoid函数代替阶跃函数，
$$ y=\frac{1}{1+e^{-z}} $$

## 线性判别分析(Linear Discriminant Analysis LDA)
1. 给定训练集，设法将样例投影到一条直线上，使得同类的投影点尽量靠近、异样的投影点尽量远离。
2. 在对新样本进行分类的时候，将其投影到同一条直线上，再根据投影点的位置来确定样本的类别。
![LDA](https://z4a.net/images/2023/12/05/LDA.png)

## 多分类任务
1. 将问题分解成多个二分类问题，对问题进行拆分，为拆出的每个二分类任务训练一个分类器。
2. 在测试的时候，对这些分类器的结果进行集成，最终获得多分类结果。
3. 拆分策略一般分为三种：
   1. 一对一(One vs. One ,Ovo)
   2. 一对其余(One vs. Rest,OvR)
   3. 多对多(Many vs. Many,MvM)

OvO: 将N个类别两两配对，得到N*(N-1)/2个二分类任务


OvR: 每次将一个类作为正类，其余作为反例，一共训练N个分类器。测试的时候若仅有一个分类器预测为正类，则把该结果作为最终分类结果。

![OvO,OvR](https://z4a.net/images/2023/12/05/OvO-OvR.png)

1. 通过上图，很容易看出。OvR只需要训练N个分类器，OvO需要训练N*(N-1)/2个分类器。
2. OvO的存储开销和测试时间通常都比OvR要大。
3. 但是在训练时，OvR每个分类器都要用到所有的训练样例，OvO每个分类器仅用到两个类的样例，因此在类别多的时候，OvO训练时间开销比OvR小。
4. MvM是每次将若干个类作为正类，其他若干个类作为反类
5. MvM的正反类的构造必须有特殊的设计，不能随意取。